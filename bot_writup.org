Machine learn some Dota.
* Introduction
Creating a custom machine learning solution. The obvious step after
writing a custom Dota bot is to teach it to teach itself.
* Distributed Dota as a service
** docker as a function
We needed to run a Dota executable multiple times with different bot
files to determine which set of weights were the most effective in
practice. Docker was the obvious choice. Setting up a docker container
that grabs the Dota and bot files from the local disk was
simple. After some googling and testing we had a way to run a game of
Dota and pipe the output to a file.
#+BEGIN_SRC
FROM fedora:latest
COPY . /scripts
VOLUME /dota
CMD sh /scripts/run_game.sh
#+END_SRC
** parsing game logs
Of course Dota couldn't provide nice json files as logs so let's get
to parsing. Dota game logs vary in length but are consistent with
spacing and formatting. For example, if a tower dies the log will
always read: =Building: npc_dota_3_tower destroyed=. Man if only there
were a way to use regular expressions to match lines and grab fields
from those lines.
*** awk
AWK is a specific purpose programming language (like wolfram alpha or
bison) as opposed to a general purpose programming language (like Lisp
or Prolog). This means that AWK does one thing, and one thing well;
text stream processing. Using Gnu awk we can easily write statement that
will write the time a tower dies.
#+BEGIN_SRC awk
  if($0 ~ /Building: npc_dota_.*_(tower|rax)/){
      printf "tower %s died at %d", $2, systime()
  }
#+END_SRC
Using this as the basis, it is a non issue to implement every other
action that can happen throughout the game.
** distributed Dota
We can run multiple games and pipe the nicely formatted output to
files. Now we would like to run hundreds of games and use the data
from them to create pretty graphs representing how well our bot
does. each docker container runs one game of Dota, uses two cores, and
about 1.3 gigs of ram. Since our computers have 8 threads and 8 gigs
of ram we can optimally run 4 games at a time. What we would like to
do is tell the server to run 100 games and have each computer run up
to 4 games in parallel until that 100 game quota is met.
*** coroutines are cool
We will build our own custom solution in a programming language made
for distributed computing. Our general layout will work like this.
**** server
The server is informed that 100 games are to be played. The server
then starts each worker and opens a http server to listen for requests
from each worker. It listens for a START request and replies yes or no
depending on if there are still games to be run or not. If it receives
a json file instead of a start request it will log it and continue on.
The server is just smart enough to worry about how many games it is
running and how many games it needs to run, nothing else. Load
balancing is handled by the worker.
#+BEGIN_SRC go
  // start http server
  finished.Add(1)
  go httpListener()

  // spin up workers
  for i, worker := range(workers) {
	  fmt.Printf("Starting worker %s\n", worker)
	  err := startWorker(worker)
	  if err != nil {
		  log.Print(err)
	  }
  }
  // wait for every worker to complete their games
  finished.Wait()
#+END_SRC
finished is a wait group meaning that finished.Wait() will block until
every worker has sent back it's json data.
**** worker
The worker is in charge of load balancing it's computer and running
games.  When the server is woke it sends MAX_GAMES requests to the
server asking to start games. In Go this is easy
#+BEGIN_SRC go
  limit := limiter.NewConcurrencyLimiter(MAX_GAMES)
  done := false
  for !done {
	  done = limit.Execute(runGame)
	  time.Sleep(time.Second * 3) // because other computers are slow
  }
  limit.Wait()
#+END_SRC
runGame sends a http request to the server and runs a game based on
the response. since running a docker container/Dota game, blocks until
it has finished running we need to spawn a new process every time we
start a game. Again, in Go this is easy.

Now that we have our server and workers communicating nicely we can
run lots of games and generate huge amounts of data.
** machine learning
As fun as tweaking random bot variables is, it's not very
efficient.. for humans. Luckily computers can do boring tasks very
fast. Since we have a lack of time and processing power we will use an
algorithm that takes less of both.
*** genetic algorithms
Genetic algorithms are a type of machine learning based on random
mutations and survival of the fittest. Instead of running the server
once and getting a result back, we want that resul to be fed back into
next round's bots. The new server will work as follows:
*** awk
AWK is a specific purpose programming language (like wolfram alpha or
bison) as opposed to a general purpose programming language (like Lisp
or Prolog). This means that AWK does one thing, and one thing well;
text stream processing. Using Gnu awk we can easily write statement that
will write the time a tower dies.
#+BEGIN_SRC awk
  if($0 ~ /Building: npc_dota_.*_(tower|rax)/){
      printf "tower %s died at %d", $2, systime()
  }
#+END_SRC
Using this as the basis, it is a non issue to implement every other
action that can happen throughout the game.
** distributed Dota
We can run multiple games and pipe the nicely formatted output to
files. Now we would like to run hundreds of games and use the data
from them to create pretty graphs representing how well our bot
does. each docker container runs one game of Dota, uses two cores, and
about 1.3 gigs of ram. Since our computers have 8 threads and 8 gigs
of ram we can optimally run 4 games at a time. What we would like to
do is tell the server to run 100 games and have each computer run up
to 4 games in parallel until that 100 game quota is met.
*** coroutines are cool
We will build our own custom solution in a programming language made
for distributed computing. Our general layout will work like this.
**** server
The server is informed that 100 games are to be played. The server
then starts each worker and opens a http server to listen for requests
from each worker. It listens for a START request and replies yes or no
depending on if there are still games to be run or not. If it receives
a json file instead of a start request it will log it and continue on.
The server is just smart enough to worry about how many games it is
running and how many games it needs to run, nothing else. Load
balancing is handled by the worker.
#+BEGIN_SRC go
  // start http server
  finished.Add(1)
  go httpListener()

  // spin up workers
  for i, worker := range(workers) {
	  fmt.Printf("Starting worker %s\n", worker)
	  err := startWorker(worker)
	  if err != nil {
		  log.Print(err)
	  }
  }
  // wait for every worker to complete their games
  finished.Wait()
#+END_SRC
finished is a wait group meaning that finished.Wait() will block until
every worker has sent back it's json data.
**** worker
The worker is in charge of load balancing it's computer and running
games.  When the server is woke it sends MAX_GAMES requests to the
server asking to start games. In Go this is easy
#+BEGIN_SRC go
  limit := limiter.NewConcurrencyLimiter(MAX_GAMES)
  done := false
  for !done {
	  done = limit.Execute(runGame)
	  time.Sleep(time.Second * 3) // because other computers are slow
  }
  limit.Wait()
#+END_SRC
runGame sends a http request to the server and runs a game based on
the response. since running a docker container/Dota game, blocks until
it has finished running we need to spawn a new process every time we
start a game. Again, in Go this is easy.

Now that we have our server and workers communicating nicely we can
run lots of games and generate huge amounts of data.
** machine learning
As fun as tweaking random bot variables is, it's not very
efficient.. for humans. Luckily computers can do boring tasks very
fast. Since we have a lack of time and processing power we will use an
algorithm that takes less of both.
*** genetic algorithms
Genetic algorithms are a type of machine learning based on random
mutations and survival of the fittest. Instead of running the server
once and getting a result back, we want that resul to be fed back into
next round's bots. The new server will work as follows:
*** awk
AWK is a specific purpose programming language (like wolfram alpha or
bison) as opposed to a general purpose programming language (like Lisp
or Prolog). This means that AWK does one thing, and one thing well;
text stream processing. Using Gnu awk we can easily write statement that
will write the time a tower dies.
#+BEGIN_SRC awk
  if($0 ~ /Building: npc_dota_.*_(tower|rax)/){
      printf "tower %s died at %d", $2, systime()
  }
#+END_SRC
Using this as the basis, it is a non issue to implement every other
action that can happen throughout the game.
** distributed Dota
We can run multiple games and pipe the nicely formatted output to
files. Now we would like to run hundreds of games and use the data
from them to create pretty graphs representing how well our bot
does. each docker container runs one game of Dota, uses two cores, and
about 1.3 gigs of ram. Since our computers have 8 threads and 8 gigs
of ram we can optimally run 4 games at a time. What we would like to
do is tell the server to run 100 games and have each computer run up
to 4 games in parallel until that 100 game quota is met.
*** coroutines are cool
We will build our own custom solution in a programming language made
for distributed computing. Our general layout will work like this.
**** server
The server is informed that 100 games are to be played. The server
then starts each worker and opens a http server to listen for requests
from each worker. It listens for a START request and replies yes or no
depending on if there are still games to be run or not. If it receives
a json file instead of a start request it will log it and continue on.
The server is just smart enough to worry about how many games it is
running and how many games it needs to run, nothing else. Load
balancing is handled by the worker.
#+BEGIN_SRC go
  // start http server
  finished.Add(1)
  go httpListener()

  // spin up workers
  for i, worker := range(workers) {
	  fmt.Printf("Starting worker %s\n", worker)
	  err := startWorker(worker)
	  if err != nil {
		  log.Print(err)
	  }
  }
  // wait for every worker to complete their games
  finished.Wait()
#+END_SRC
finished is a wait group meaning that finished.Wait() will block until
every worker has sent back it's json data.
**** worker
The worker is in charge of load balancing it's computer and running
games.  When the server is woke it sends MAX_GAMES requests to the
server asking to start games. In Go this is easy
#+BEGIN_SRC go
  limit := limiter.NewConcurrencyLimiter(MAX_GAMES)
  done := false
  for !done {
	  done = limit.Execute(runGame)
	  time.Sleep(time.Second * 3) // because other computers are slow
  }
  limit.Wait()
#+END_SRC
runGame sends a http request to the server and runs a game based on
the response. since running a docker container/Dota game, blocks until
it has finished running we need to spawn a new process every time we
start a game. Again, in Go this is easy.

Now that we have our server and workers communicating nicely we can
run lots of games and generate huge amounts of data.
** machine learning
As fun as tweaking random bot variables is, it's not very
efficient.. for humans. Luckily computers can do boring tasks very
fast. Since we have a lack of time and processing power we will use an
algorithm that takes less of both.
*** genetic algorithms
Genetic algorithms are a type of machine learning based on random
mutations and survival of the fittest. Instead of running the server
once and getting a result back, we want that resul to be fed back into
next round's bots. The new server will work as follows:
*** awk
AWK is a specific purpose programming language (like wolfram alpha or
bison) as opposed to a general purpose programming language (like Lisp
or Prolog). This means that AWK does one thing, and one thing well;
text stream processing. Using Gnu awk we can easily write statement that
will write the time a tower dies.
#+BEGIN_SRC awk
  if($0 ~ /Building: npc_dota_.*_(tower|rax)/){
      printf "tower %s died at %d", $2, systime()
  }
#+END_SRC
Using this as the basis, it is a non issue to implement every other
action that can happen throughout the game.
** distributed Dota
We can run multiple games and pipe the nicely formatted output to
files. Now we would like to run hundreds of games and use the data
from them to create pretty graphs representing how well our bot
does. each docker container runs one game of Dota, uses two cores, and
about 1.3 gigs of ram. Since our computers have 8 threads and 8 gigs
of ram we can optimally run 4 games at a time. What we would like to
do is tell the server to run 100 games and have each computer run up
to 4 games in parallel until that 100 game quota is met.
*** coroutines are cool
We will build our own custom solution in a programming language made
for distributed computing. Our general layout will work like this.
**** server
The server is informed that 100 games are to be played. The server
then starts each worker and opens a http server to listen for requests
from each worker. It listens for a START request and replies yes or no
depending on if there are still games to be run or not. If it receives
a json file instead of a start request it will log it and continue on.
The server is just smart enough to worry about how many games it is
running and how many games it needs to run, nothing else. Load
balancing is handled by the worker.
#+BEGIN_SRC go
  // start http server
  finished.Add(1)
  go httpListener()

  // spin up workers
  for i, worker := range(workers) {
	  fmt.Printf("Starting worker %s\n", worker)
	  err := startWorker(worker)
	  if err != nil {
		  log.Print(err)
	  }
  }
  // wait for every worker to complete their games
  finished.Wait()
#+END_SRC
finished is a wait group meaning that finished.Wait() will block until
every worker has sent back it's json data.
**** worker
The worker is in charge of load balancing it's computer and running
games.  When the server is woke it sends MAX_GAMES requests to the
server asking to start games. In Go this is easy
#+BEGIN_SRC go
  limit := limiter.NewConcurrencyLimiter(MAX_GAMES)
  done := false
  for !done {
	  done = limit.Execute(runGame)
	  time.Sleep(time.Second * 3) // because other computers are slow
  }
  limit.Wait()
#+END_SRC
runGame sends a http request to the server and runs a game based on
the response. since running a docker container/Dota game, blocks until
it has finished running we need to spawn a new process every time we
start a game. Again, in Go this is easy.

Now that we have our server and workers communicating nicely we can
run lots of games and generate huge amounts of data.
** machine learning
As fun as tweaking random bot variables is, it's not very
efficient.. for humans. Luckily computers can do boring tasks very
fast. Since we have a lack of time and processing power we will use an
algorithm that takes less of both.
*** genetic algorithms
Genetic algorithms are a type of machine learning based on random
mutations and survival of the fittest. Instead of running the server
once and getting a result back, we want that resul to be fed back into
next round's bots. The new server will work as follows:
#+BEGIN_SRC plantuml :file genetic_algo.png
  partition "Genetic Algorithm" {
	  while (Gene pool)
	  :Randomly mutate each gene
	  to create new genes;
	  :Play games with each gene;
	  partition "Process Data From Games" {
		  if (Gene is good?) then (yes)
		  :High probability of being
		  added to the gene pool;
		  else (no)
		  :Low probability of being
		  added to the gene pool;
		  endif
		  :add genes back to pool;
		  note right
		  The genes that perform better
		  will most likely be added to
		  the pool but <b>all</b> genes
		  have a chance to be added
		  end note
	  }
	  endwhile
	  :profit?;
  }
#+END_SRC

#+RESULTS:
[[file:genetic_algo.png]]
